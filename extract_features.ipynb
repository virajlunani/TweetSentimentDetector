{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/dorayguo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from extract_feats import *\n",
    "from hydrate_tweets import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i didnt feel humiliated</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i can go from feeling so hopeless to so damned...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am feeling grouchy</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15995</th>\n",
       "      <td>i just had a very brief time in the beanbag an...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15996</th>\n",
       "      <td>i am now turning and i feel pathetic that i am...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15997</th>\n",
       "      <td>i feel strong and good overall</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15998</th>\n",
       "      <td>i feel like this was such a rude comment and i...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15999</th>\n",
       "      <td>i know a lot but i feel so stupid because i ca...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "0                                i didnt feel humiliated      0\n",
       "1      i can go from feeling so hopeless to so damned...      0\n",
       "2       im grabbing a minute to post i feel greedy wrong      3\n",
       "3      i am ever feeling nostalgic about the fireplac...      2\n",
       "4                                   i am feeling grouchy      3\n",
       "...                                                  ...    ...\n",
       "15995  i just had a very brief time in the beanbag an...      0\n",
       "15996  i am now turning and i feel pathetic that i am...      0\n",
       "15997                     i feel strong and good overall      1\n",
       "15998  i feel like this was such a rude comment and i...      3\n",
       "15999  i know a lot but i feel so stupid because i ca...      0\n",
       "\n",
       "[16000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/training.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = {0: 'sadness', 1: 'joy', 2: 'love', 3: 'anger', 4: 'fear'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfeatures = {'anger': 0, 'anticipation': 1, 'disgust': 2, 'fear': 3, 'joy': 4, 'sadness': 5,\\n            'surprise': 6, 'trust': 7, 'negative': 8, 'positive': 9, 'CC': 10, 'IN': 11, 'JJR': 12,\\n            'JJS': 13, 'PRP': 14, '!': 15, '?': 16}\\nnrc_pos_feats = extract_nrc_pos_feats(df, features)\\nnrc_pos_feats\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "features = {'anger': 0, 'anticipation': 1, 'disgust': 2, 'fear': 3, 'joy': 4, 'sadness': 5,\n",
    "            'surprise': 6, 'trust': 7, 'negative': 8, 'positive': 9, 'CC': 10, 'IN': 11, 'JJR': 12,\n",
    "            'JJS': 13, 'PRP': 14, '!': 15, '?': 16}\n",
    "nrc_pos_feats = extract_nrc_pos_feats(df, features)\n",
    "nrc_pos_feats\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>didnt feel humili</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>go feel hopeless damn hope around someon care ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im grab minut post feel greedi wrong</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ever feel nostalg fireplac know still properti</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>feel grouchi</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15995</th>\n",
       "      <td>veri brief time beanbag said anna feel like be...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15996</th>\n",
       "      <td>turn feel pathet still wait tabl sub teach degre</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15997</th>\n",
       "      <td>feel strong good overal</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15998</th>\n",
       "      <td>feel like thi wa rude comment im glad</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15999</th>\n",
       "      <td>know lot feel stupid becaus portray</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "0                                      didnt feel humili      0\n",
       "1      go feel hopeless damn hope around someon care ...      0\n",
       "2                   im grab minut post feel greedi wrong      3\n",
       "3         ever feel nostalg fireplac know still properti      2\n",
       "4                                           feel grouchi      3\n",
       "...                                                  ...    ...\n",
       "15995  veri brief time beanbag said anna feel like be...      0\n",
       "15996   turn feel pathet still wait tabl sub teach degre      0\n",
       "15997                            feel strong good overal      1\n",
       "15998              feel like thi wa rude comment im glad      3\n",
       "15999                know lot feel stupid becaus portray      0\n",
       "\n",
       "[16000 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_data = cleanData(df)\n",
    "cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, vectorizer = createBoWFeatureVec(cleaned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]]),\n",
       " array([0, 0, 3, ..., 1, 3, 0]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2, y, vectorizer2 = createTfIdfFeatureVec(cleaned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]),\n",
       " array([0, 0, 3, ..., 1, 3, 0]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = splitData(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2, X_test2, y_train2, y_test2 = splitData(X2,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf, accuracy = trainLogisticRegression(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2, accuracy2 = trainLogisticRegression(X_train2, X_test2, y_train2, y_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.853"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy 'using BoW'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.817"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy2 'using TfIdF'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1240727808080412673</td>\n",
       "      <td>0.357143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1240727808005079041</td>\n",
       "      <td>0.116071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1240727808340414464</td>\n",
       "      <td>-0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1240727808629813248</td>\n",
       "      <td>-0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1240727808617230336</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>831322</th>\n",
       "      <td>1240861453524930562</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>831323</th>\n",
       "      <td>1240861453554315265</td>\n",
       "      <td>0.140693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>831324</th>\n",
       "      <td>1240861453503959043</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>831325</th>\n",
       "      <td>1240861453311070209</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>831326</th>\n",
       "      <td>1240861453680128000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>831327 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          0         1\n",
       "0       1240727808080412673  0.357143\n",
       "1       1240727808005079041  0.116071\n",
       "2       1240727808340414464 -0.050000\n",
       "3       1240727808629813248 -0.714286\n",
       "4       1240727808617230336  0.700000\n",
       "...                     ...       ...\n",
       "831322  1240861453524930562  0.000000\n",
       "831323  1240861453554315265  0.140693\n",
       "831324  1240861453503959043  0.000000\n",
       "831325  1240861453311070209  0.000000\n",
       "831326  1240861453680128000  0.000000\n",
       "\n",
       "[831327 rows x 2 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/corona_tweets/corona_tweets_01.csv', header=None)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['longer defend make excuse reasoning want intentional racist',\n",
       "  'everyone looks sick',\n",
       "  'good came china',\n",
       "  'corona day feels like sunday ...',\n",
       "  'close president notes seen crossed corona replaced chinese virus speaks coronavirus task force today white house',\n",
       "  'pretty much corona virus germ wants entered body passed hours bodied shots henny',\n",
       "  'china irresponsible criminal behavior wake covid leave weakest global position memory u emerge stronger china must held fully accountable harm unleashed planet',\n",
       "  'pls rt dear news press conferences around country corona virus emergency sign language interpreters standing next speakers providing access millions deaf americans please',\n",
       "  \"lets start resource thread loans grants small businesses affected corona we'll start please share disaster loan assistance sba sign loan help get times\",\n",
       "  'nasty flu went round december early jan telling corona bare ppl sick including complaining know symptoms corona',\n",
       "  'corona day feels like sunday ...',\n",
       "  'corona dangerous come post offic',\n",
       "  'jamila think measures put place government prevent us getting point start community transmissions',\n",
       "  'yes hell jack donated 500,000 testing kits million masks',\n",
       "  \"among things shows they're using term coronavirus internally like everyone else throwing publicly would rather argument political correctness discuss response\",\n",
       "  \"they're trying spur culture war distract incompetence notice firing professionals protect us terrorist threats disbanding offices prepare pandemics making decisions benefit businesses\",\n",
       "  'dick bigger corona virus dick bigger corona virus dick bigger corona virus dick bigger corona virus dick bigger corona virus dick bigger corona virus see dumb sound',\n",
       "  'funeral hear messi dropped 6.2 rating sofascore',\n",
       "  'everyone looks sick',\n",
       "  'weeks fucking global pandemic hamstring strain',\n",
       "  'proof intentional tactic trump et al trying bait us fighting racism instead incompetence followers either see mind racism ineptitude deeply troubling',\n",
       "  'generation z want name folks love calling everybody millennials millennials spring break millennials home yelling boomer parents sit still faith fear',\n",
       "  'cldn wind day without visiting check health progress covid pts glad see support teams give challenging situation hats dedication support medical fraternity.we shall overcome cvb',\n",
       "  'vain eschew wearing glasses obviously needs font big glaring photographic evidence racism',\n",
       "  'holy shit fake news',\n",
       "  'man save us corona virus',\n",
       "  'admiring dealing customers since whole corona virus started however today got phone call travel agent booked holiday saying offer us voucher get refund',\n",
       "  'disgusting let clutch pearls bc riff ppl actively killing americans',\n",
       "  'photo briefing papers today shows corona crossed chinese written sharpie handwriting describe virus description said today racist',\n",
       "  'weeks fucking global pandemic hamstring strain',\n",
       "  'year old stepmom admitted memorial hospital belleville illinois last night fever coughing said test corona virus traveled talked day prior nothing hell wrong age',\n",
       "  'kicked usg students returned cities open corona cases sav cases usg wants everyone return sav get stuff risking bringing corona city school :thinking_face: still letting usg make decisions',\n",
       "  'holy shit hate every person video',\n",
       "  'must bring taiwan fully date taiwan around cases despite proximity china global response china must stop playing geopolitics global health thoughts',\n",
       "  'get corona get corona said one ohio student spring break florida end day gonna let stop partying via',\n",
       "  'kano state emergency contacts case suspected corona virus outbreak dr imam wada bello dr bashir lawan muhammad sulaiman ilyasu coordinator kano dr sharif yahaya musa rt awareness',\n",
       "  'longer defend make excuse reasoning want intentional racist',\n",
       "  'generation z want name folks love calling everybody millennials millennials spring break millennials home yelling boomer parents sit still faith fear',\n",
       "  'generation z want name folks love calling everybody millennials millennials spring break millennials home yelling boomer parents sit still faith fear',\n",
       "  'repugnant',\n",
       "  'everyone coming corona barista',\n",
       "  'nasty flu went round december early jan telling corona bare ppl sick including complaining know symptoms corona',\n",
       "  'year old pasadena first person die corona visiting florida shame disneyland closing sooner smh known something like would happen recent measles outbreak happened anaheim recently',\n",
       "  'niggas walking club corona epidemic end',\n",
       "  'everything fine world liverpool decided take premier league title :weary_face: :broken_heart: :face_with_tears_of_joy: kobe died :weary_face: world war corona virus :weary_face: :loudly_crying_face: :broken_heart:',\n",
       "  'sweet potatoes beans porridge garri dinner served :man_cook:',\n",
       "  'quarantine kicked depression couple notches thanks work routine void im effectively avoiding phone bc everyone nuts sending corona stuff dont blame christ anxiety driving wal',\n",
       "  'deleted scene twd special corona virus',\n",
       "  'man piss could stop corona',\n",
       "  'nasty flu went round december early jan telling corona bare ppl sick including complaining know symptoms corona',\n",
       "  'close president notes seen crossed corona replaced chinese virus speaks coronavirus task force today white house',\n",
       "  'generation z want name folks love calling everybody millennials millennials spring break millennials home yelling boomer parents sit still faith fear',\n",
       "  'nasty flu went round december early jan telling corona bare ppl sick including complaining know symptoms corona',\n",
       "  \"among things shows they're using term coronavirus internally like everyone else throwing publicly would rather argument political correctness discuss response\",\n",
       "  'asked dc cop noticed since coronavirus sent people home domestic violence said without missing beat',\n",
       "  'lot would actually benefit taking good look introspectively issues women gross',\n",
       "  'close president notes seen crossed corona replaced chinese virus speaks coronavirus task force today white house',\n",
       "  \"among things shows they're using term coronavirus internally like everyone else throwing publicly would rather argument political correctness discuss response\",\n",
       "  'generation z want name folks love calling everybody millennials millennials spring break millennials home yelling boomer parents sit still faith fear'],\n",
       " ['trying use wits good evil people united change world better resist resiliently pro-science logic get vaccinated',\n",
       "  'starchild go wild :dizzy: :butterfly: ig remingtonxx :herb:',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  'instagram :scissors:',\n",
       "  'conectada con el amor la naturaleza',\n",
       "  \"i'm deaf fans bloodline paul heyman fans wwe love fans anoa family acknowledge tribal chief roman reigns :red_heart: \\u200d:fire: :black_heart: :blue_heart: :white_heart: :drop_of_blood: :index_pointing_up_medium-light_skin_tone: :call_me_hand_medium-light_skin_tone: :love-you_gesture_medium-light_skin_tone:\",\n",
       "  'ig making music games em right :eight-pointed_star: ',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  'democrat living red state old enough seen try bs original acct got suspended error starting pain',\n",
       "  'mslis unattributed art accessed carpe librum русский корабль иди на хуй',\n",
       "  'former deputy assistant secretary external affairs us department labor current small business owner',\n",
       "  'avi beautiful make sure follow tribute :smiling_face_with_horns: dm inquiries premium promo services',\n",
       "  '',\n",
       "  'trying survive work',\n",
       "  'mom twin girls :butterfly:',\n",
       "  'retired proud democrat votebiden technology challenged always sure twitter works love peopl',\n",
       "  'love real estate kittens cake sac state alumni',\n",
       "  'die hard fan thala :collision: :collision: :collision: evanukaakavum thalaiyaa vittu kuduka maten ... என்றும் தல அஜித் வழியில் :smiling_face_with_heart-eyes: :smiling_face_with_heart-eyes: vaalu vaala vidu ... back id',\n",
       "  'cook soup garden basically arrogant retweet without comment like messing others words thoughts',\n",
       "  'finds remedies connects dots space geek higher orbits biohacking village bio-isac',\n",
       "  'la vie cours pour avancer et moi je dors pour la laisser rattraper :zzz: :zzz: h',\n",
       "  'flight disruption delay cancellation last yrs get upto £ compensation per person check flight find second',\n",
       "  '',\n",
       "  'ex-gop proud support troops first responders love politics 1a believe media keeps us free',\n",
       "  'wake find eyes world :skull: :rose:',\n",
       "  '',\n",
       "  'metanoia moon child :poodle:',\n",
       "  'games cartoons art',\n",
       "  'gfhk',\n",
       "  'used regional manager circuit city know ended cultural critic slusho',\n",
       "  'nonchalant cup tea diluted bro code vibes :game_die: :hot_beverage:',\n",
       "  \"right right wrong wrong respect love everyone :red_heart:  :red_heart:  trying learn walk people's shoes step feet god big\",\n",
       "  'queer trans man :no_one_under_eighteen: account like look antis radfems exclusionists assholes fuck :no_one_under_eighteen:',\n",
       "  'bibinii trying lot new things lately ...',\n",
       "  'aham',\n",
       "  'هي levantin',\n",
       "  '🇺 🇸 🇬 🇧 🇳 🇬 queen :crown:',\n",
       "  'follower pancake lord',\n",
       "  'nigga ima always nigga ima continue nigga till day die :hundred_points:',\n",
       "  ':B_button_(blood_type):🆁🅴🆉🆉 🅸🆂 🅻🅸🅵🅴 :red_heart: :red_heart:',\n",
       "  'lawyer real estate expert negotiator aspiring politician principal oar legal consult partner abayomi adetoun co die hard lfc fan abeg tag',\n",
       "  'hi im fucking epic ║ ║ furry fanartist :balloon: mostly lurker patty uris fan funny little guy enthusiast :o) commission info carrd :sparkles:',\n",
       "  '',\n",
       "  'politickin chicken 🇨 🇴 :Statue_of_Liberty: texas ex :sign_of_the_horns_medium-light_skin_tone: :balance_scale: ',\n",
       "  ':red_heart: 🇬 🇧 🇾 🇪',\n",
       "  'please father mr vision call tele',\n",
       "  'everyday improve t.byers :smiling_face_with_hearts:',\n",
       "  'insta lashay_x :airplane:  :money_bag: :graduation_cap:',\n",
       "  'energy law policy professional lesbian mom grandma democrat far perfect trying write read poetry lot days',\n",
       "  'ut-business marketing mastersmgmt aquinas proud dad wwii vfw frankjr vietnamvet purple heart recipients defendamerica',\n",
       "  '',\n",
       "  \"i'm great believer luck find harder work thomas jefferson\",\n",
       "  'pro market pro immigrant pro gmo anti superstition avatar',\n",
       "  \"batman moron i'm animation clips\"])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_fields=['text'],\n",
    "user_fields=['description']\n",
    "\n",
    "tweet_data, user_data = hydrateTweets(df, tweet_fields, user_fields)\n",
    "tweet_data, user_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/dorayguo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 1, 0]]),\n",
       " array([ 0.35714286,  0.11607143, -0.05      , ...,  0.        ,\n",
       "         0.        ,  0.        ]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = createBoWFeatureVecTweets(tweet_data, df[1].values, vectorizer)\n",
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        ...,\n",
       "        [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        , 0.19815123,\n",
       "         0.        ]]),\n",
       " array([ 0.35714286,  0.11607143, -0.05      , ...,  0.        ,\n",
       "         0.        ,  0.        ]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = createTfIdfFeatureVecTweets(tweet_data, df[1].values, vectorizer2)\n",
    "X, y"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
